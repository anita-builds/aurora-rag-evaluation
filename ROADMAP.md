# Roadmap

1) Add a small public sample policy corpus (non-sensitive) to demonstrate retrieval end to end
2) Introduce reranking and compare retrieval quality on difficult queries
3) Improve evaluation beyond simple heuristics, using a clear rubric and measurable pass criteria
4) Add reproducible packaging and a one-command run path (env setup, dependencies, and scripts)
5) Expand test coverage with paraphrases, adversarial prompts, and regression cases
